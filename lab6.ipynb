{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "\n",
    "    def is_fully_expanded(self, action_space):\n",
    "        return len(self.children) == action_space.n\n",
    "\n",
    "    def best_child(self, exploration_constant=math.sqrt(2)):\n",
    "        choices_weights = [\n",
    "            (child.value / (child.visits + 1e-10)) + exploration_constant * math.sqrt(\n",
    "                math.log(self.visits + 1) / (child.visits + 1e-10)\n",
    "            )\n",
    "            for child in self.children.values()\n",
    "        ]\n",
    "        return list(self.children.values())[np.argmax(choices_weights)]\n",
    "\n",
    "    def select_best_action(self):\n",
    "        best_action = max(self.children.items(), key=lambda item: item[1].visits)[0]\n",
    "        return best_action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, env, num_simulations=1000, exploration_constant=math.sqrt(2)):\n",
    "        self.env = env\n",
    "        self.num_simulations = num_simulations\n",
    "        self.exploration_constant = exploration_constant\n",
    "\n",
    "    def search(self, initial_state):\n",
    "        root = Node(initial_state)\n",
    "\n",
    "        for _ in range(self.num_simulations):\n",
    "            node = root\n",
    "            state = initial_state\n",
    "\n",
    "            # Selection\n",
    "            while not node.is_fully_expanded(self.env.action_space) and node.children:\n",
    "                node = node.best_child(self.exploration_constant)\n",
    "                action = list(node.parent.children.keys())[list(node.parent.children.values()).index(node)]\n",
    "                state, _, done, _, _ = self.env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Expansion\n",
    "            if not node.is_fully_expanded(self.env.action_space) and not done:\n",
    "                action = random.choice([a for a in range(self.env.action_space.n) if a not in node.children])\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                node.children[action] = Node(next_state, parent=node)\n",
    "                node = node.children[action]\n",
    "\n",
    "            # Simulation\n",
    "            reward = self.rollout(state)\n",
    "\n",
    "            # Backpropagation\n",
    "            while node is not None:\n",
    "                node.visits += 1\n",
    "                node.value += reward\n",
    "                node = node.parent\n",
    "\n",
    "        return root.select_best_action()\n",
    "\n",
    "    def rollout(self, state):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()\n",
    "            state, reward, done, _, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "        return total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'done' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 13\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     15\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mMCTS.search\u001b[1;34m(self, initial_state)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Expansion\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_fully_expanded(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdone\u001b[49m:\n\u001b[0;32m     24\u001b[0m     action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn) \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren])\n\u001b[0;32m     25\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'done' referenced before assignment"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "mcts = MCTS(env)\n",
    "\n",
    "episodes = 100\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = mcts.search(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "plt.plot()\n",
    "print(f'Average Reward over {episodes}: {np.mean(rewards)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
